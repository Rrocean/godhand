#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯• - æµ‹é‡æ ¸å¿ƒæ¨¡å—æ€§èƒ½
"""

import sys
import time
import statistics
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import tempfile
import os

sys.path.insert(0, str(Path(__file__).parent.parent))

from PIL import Image

from core import (
    VisualEngine, TaskPlanner, LearningSystem,
    ElementLibrary, SmartParser
)
from core.visual_engine import UIElement, ElementType


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.results = {}

    def benchmark_visual_engine(self, iterations=100):
        """åŸºå‡†æµ‹è¯•è§†è§‰å¼•æ“"""
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•: VisualEngine")

        engine = VisualEngine(use_ocr=False, use_ml=False)
        screenshot = Image.new('RGB', (1920, 1080), color='white')

        # æµ‹è¯•å…ƒç´ æ£€æµ‹æ€§èƒ½
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            elements = engine.detect_buttons(screenshot)
            elapsed = (time.perf_counter() - start) * 1000  # ms
            times.append(elapsed)

        self.results['visual_engine'] = {
            'mean_ms': statistics.mean(times),
            'median_ms': statistics.median(times),
            'min_ms': min(times),
            'max_ms': max(times),
            'stdev_ms': statistics.stdev(times) if len(times) > 1 else 0
        }

        print(f"   å¹³å‡: {self.results['visual_engine']['mean_ms']:.2f}ms")
        print(f"   ä¸­ä½æ•°: {self.results['visual_engine']['median_ms']:.2f}ms")
        print(f"   æœ€å°: {self.results['visual_engine']['min_ms']:.2f}ms")
        print(f"   æœ€å¤§: {self.results['visual_engine']['max_ms']:.2f}ms")

        return self.results['visual_engine']['mean_ms'] < 100  # ç›®æ ‡ < 100ms

    def benchmark_task_planner(self, iterations=50):
        """åŸºå‡†æµ‹è¯•ä»»åŠ¡è§„åˆ’å™¨"""
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•: TaskPlanner")

        planner = TaskPlanner(use_llm=False)
        instructions = [
            "æ‰“å¼€è®°äº‹æœ¬",
            "æ‰“å¼€è®¡ç®—å™¨å¹¶è®¡ç®—1+1",
            "æ‰“å¼€æµè§ˆå™¨æœç´¢Pythonæ•™ç¨‹",
            "ç‚¹å‡»ä¿å­˜æŒ‰é’®ç„¶åå…³é—­çª—å£",
            "è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ç„¶åç‚¹å‡»ç™»å½•"
        ]

        times = []
        for instruction in instructions * (iterations // len(instructions)):
            start = time.perf_counter()
            plan = planner.plan(instruction)
            elapsed = (time.perf_counter() - start) * 1000
            times.append(elapsed)

        self.results['task_planner'] = {
            'mean_ms': statistics.mean(times),
            'median_ms': statistics.median(times),
            'min_ms': min(times),
            'max_ms': max(times)
        }

        print(f"   å¹³å‡: {self.results['task_planner']['mean_ms']:.2f}ms")
        print(f"   ä¸­ä½æ•°: {self.results['task_planner']['median_ms']:.2f}ms")

        return self.results['task_planner']['mean_ms'] < 50  # ç›®æ ‡ < 50ms

    def benchmark_smart_parser(self, iterations=100):
        """åŸºå‡†æµ‹è¯•æ™ºèƒ½è§£æå™¨"""
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•: SmartParser")

        parser = SmartParser()
        commands = [
            "æ‰“å¼€è®°äº‹æœ¬",
            "è¾“å…¥Hello World",
            "æˆªå›¾ä¿å­˜åˆ°æ¡Œé¢",
            "æ‰“å¼€è®¡ç®—å™¨è®¡ç®—1+1",
            "ç‚¹å‡»ç¡®å®šæŒ‰é’®"
        ]

        times = []
        for command in commands * (iterations // len(commands)):
            start = time.perf_counter()
            result = parser.parse(command)
            elapsed = (time.perf_counter() - start) * 1000
            times.append(elapsed)

        self.results['smart_parser'] = {
            'mean_ms': statistics.mean(times),
            'median_ms': statistics.median(times)
        }

        print(f"   å¹³å‡: {self.results['smart_parser']['mean_ms']:.2f}ms")
        print(f"   ä¸­ä½æ•°: {self.results['smart_parser']['median_ms']:.2f}ms")

        return self.results['smart_parser']['mean_ms'] < 10  # ç›®æ ‡ < 10ms

    def benchmark_element_library(self, iterations=1000):
        """åŸºå‡†æµ‹è¯•å…ƒç´ åº“"""
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•: ElementLibrary")

        with tempfile.TemporaryDirectory() as tmpdir:
            library = ElementLibrary(cache_dir=tmpdir)

            # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
            for i in range(100):
                library.add_template(
                    name=f"button_{i}",
                    element_type="button",
                    image=None,
                    text=f"Button {i}",
                    app_name="TestApp"
                )

            # æµ‹è¯•æŸ¥æ‰¾æ€§èƒ½
            times = []
            for i in range(iterations):
                start = time.perf_counter()
                results = library.find_by_text(f"Button {i % 100}", app_name="TestApp")
                elapsed = (time.perf_counter() - start) * 1000
                times.append(elapsed)

            self.results['element_library'] = {
                'mean_ms': statistics.mean(times),
                'median_ms': statistics.median(times)
            }

            print(f"   å¹³å‡: {self.results['element_library']['mean_ms']:.2f}ms")
            print(f"   ä¸­ä½æ•°: {self.results['element_library']['median_ms']:.2f}ms")

            return self.results['element_library']['mean_ms'] < 5  # ç›®æ ‡ < 5ms

    def benchmark_learning_system(self, iterations=50):
        """åŸºå‡†æµ‹è¯•å­¦ä¹ ç³»ç»Ÿ"""
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•: LearningSystem")

        learning = LearningSystem()

        # æµ‹è¯•å·¥ä½œæµå­¦ä¹ æ€§èƒ½
        times = []
        for i in range(iterations):
            start = time.perf_counter()
            demo = learning.start_demonstration(f"workflow_{i}", f"Test workflow {i}")

            for j in range(10):  # 10ä¸ªåŠ¨ä½œ
                learning.record_action(demo.id, {
                    "action": "click",
                    "target": f"button_{j}"
                })

            learning.end_demonstration(demo.id)
            elapsed = (time.perf_counter() - start) * 1000
            times.append(elapsed)

        self.results['learning_system'] = {
            'mean_ms': statistics.mean(times),
            'median_ms': statistics.median(times)
        }

        print(f"   å¹³å‡: {self.results['learning_system']['mean_ms']:.2f}ms")
        print(f"   ä¸­ä½æ•°: {self.results['learning_system']['median_ms']:.2f}ms")

        return self.results['learning_system']['mean_ms'] < 100  # ç›®æ ‡ < 100ms

    def benchmark_concurrent_operations(self, workers=4, iterations_per_worker=25):
        """åŸºå‡†æµ‹è¯•å¹¶å‘æ“ä½œ"""
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•: å¹¶å‘æ“ä½œ")

        parser = SmartParser()
        commands = ["æ‰“å¼€è®°äº‹æœ¬", "è¾“å…¥Hello", "æˆªå›¾", "ç‚¹å‡»ç¡®å®š"] * iterations_per_worker

        def parse_batch(batch):
            results = []
            for cmd in batch:
                start = time.perf_counter()
                result = parser.parse(cmd)
                elapsed = (time.perf_counter() - start) * 1000
                results.append(elapsed)
            return results

        # åˆ†å‰²ä»»åŠ¡
        batch_size = len(commands) // workers
        batches = [commands[i:i+batch_size] for i in range(0, len(commands), batch_size)]

        start = time.perf_counter()
        with ThreadPoolExecutor(max_workers=workers) as executor:
            all_results = list(executor.map(parse_batch, batches))
        total_elapsed = (time.perf_counter() - start) * 1000

        all_times = [t for batch in all_results for t in batch]

        self.results['concurrent'] = {
            'total_ms': total_elapsed,
            'mean_ms': statistics.mean(all_times),
            'throughput': len(commands) / (total_elapsed / 1000)  # ops/sec
        }

        print(f"   æ€»æ—¶é—´: {self.results['concurrent']['total_ms']:.2f}ms")
        print(f"   å¹³å‡: {self.results['concurrent']['mean_ms']:.2f}ms")
        print(f"   ååé‡: {self.results['concurrent']['throughput']:.2f} ops/sec")

        return self.results['concurrent']['throughput'] > 100  # ç›®æ ‡ > 100 ops/sec

    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("="*70)

        for name, metrics in self.results.items():
            print(f"\n{name}:")
            for metric, value in metrics.items():
                if isinstance(value, float):
                    print(f"   {metric}: {value:.3f}")
                else:
                    print(f"   {metric}: {value}")

        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(__file__).parent / "benchmark_report.json"
        import json
        with open(report_path, 'w') as f:
            json.dump(self.results, f, indent=2)

        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("\n" + "ğŸš€"*35)
        print("\n  GodHand æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("\n" + "ğŸš€"*35)

        results = []

        results.append(("VisualEngine", self.benchmark_visual_engine()))
        results.append(("TaskPlanner", self.benchmark_task_planner()))
        results.append(("SmartParser", self.benchmark_smart_parser()))
        results.append(("ElementLibrary", self.benchmark_element_library()))
        results.append(("LearningSystem", self.benchmark_learning_system()))
        results.append(("Concurrent", self.benchmark_concurrent_operations()))

        self.generate_report()

        print("\n" + "="*70)
        print("æµ‹è¯•ç»“æœæ±‡æ€»:")
        print("="*70)

        passed = sum(1 for _, r in results if r)
        for name, result in results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"   {name:<20} {status}")

        print(f"\næ€»è®¡: {passed}/{len(results)} é€šè¿‡")
        print("="*70 + "\n")

        return passed == len(results)


def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    success = benchmark.run_all_benchmarks()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
