#!/usr/bin/env python3
"""
VisualEngine ğŸ‘ï¸ - ä¸–ç•Œçº§çš„è§†è§‰ç†è§£å¼•æ“

æ ¸å¿ƒèƒ½åŠ›ï¼š
1. UI å…ƒç´ æ£€æµ‹ï¼ˆæŒ‰é’®ã€è¾“å…¥æ¡†ã€èœå•ç­‰ï¼‰
2. è¯­ä¹‰åŒ–å…ƒç´ å®šä½ï¼ˆè‡ªç„¶è¯­è¨€æè¿° â†’ å±å¹•åæ ‡ï¼‰
3. åœºæ™¯ç†è§£ï¼ˆå½“å‰åº”ç”¨çŠ¶æ€è¯†åˆ«ï¼‰
4. OCR æ–‡æœ¬è¯†åˆ«

Author: GodHand Team
Version: 1.0.0
"""

import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional, Any, Union
from enum import Enum, auto
import json
import os
from pathlib import Path


class ElementType(Enum):
    """UI å…ƒç´ ç±»å‹"""
    BUTTON = "button"           # æŒ‰é’®
    INPUT = "input"             # è¾“å…¥æ¡†
    CHECKBOX = "checkbox"       # å¤é€‰æ¡†
    RADIO = "radio"             # å•é€‰æ¡†
    DROPDOWN = "dropdown"       # ä¸‹æ‹‰æ¡†
    MENU = "menu"               # èœå•
    ICON = "icon"               # å›¾æ ‡
    TEXT = "text"               # æ–‡æœ¬
    IMAGE = "image"             # å›¾ç‰‡
    WINDOW = "window"           # çª—å£
    DIALOG = "dialog"           # å¯¹è¯æ¡†
    UNKNOWN = "unknown"         # æœªçŸ¥


@dataclass
class UIElement:
    """UI å…ƒç´ """
    type: ElementType
    x: int                      # ä¸­å¿ƒç‚¹ X
    y: int                      # ä¸­å¿ƒç‚¹ Y
    width: int
    height: int
    confidence: float           # æ£€æµ‹ç½®ä¿¡åº¦ 0-1
    text: str = ""              # å…ƒç´ ä¸Šçš„æ–‡æœ¬
    description: str = ""       # å…ƒç´ æè¿°
    attributes: Dict[str, Any] = field(default_factory=dict)  # é¢å¤–å±æ€§

    @property
    def bbox(self) -> Tuple[int, int, int, int]:
        """è¿”å›è¾¹ç•Œæ¡† (x1, y1, x2, y2)"""
        half_w = self.width // 2
        half_h = self.height // 2
        return (
            self.x - half_w,
            self.y - half_h,
            self.x + half_w,
            self.y + half_h
        )

    @property
    def area(self) -> int:
        """å…ƒç´ é¢ç§¯"""
        return self.width * self.height

    def contains_point(self, px: int, py: int) -> bool:
        """æ£€æŸ¥ç‚¹æ˜¯å¦åœ¨å…ƒç´ å†…"""
        x1, y1, x2, y2 = self.bbox
        return x1 <= px <= x2 and y1 <= py <= y2

    def distance_to(self, x: int, y: int) -> float:
        """è®¡ç®—åˆ°ç‚¹çš„è·ç¦»"""
        return np.sqrt((self.x - x) ** 2 + (self.y - y) ** 2)

    def to_dict(self) -> Dict:
        return {
            "type": self.type.value,
            "x": self.x,
            "y": self.y,
            "width": self.width,
            "height": self.height,
            "confidence": self.confidence,
            "text": self.text,
            "description": self.description,
            "attributes": self.attributes
        }


@dataclass
class SceneContext:
    """åœºæ™¯ä¸Šä¸‹æ–‡"""
    application: str            # å½“å‰åº”ç”¨åç§°
    window_title: str           # çª—å£æ ‡é¢˜
    window_size: Tuple[int, int]  # çª—å£å°ºå¯¸
    scene_type: str             # åœºæ™¯ç±»å‹ï¼ˆç™»å½•é¡µ/ä¸»ç•Œé¢/å¼¹çª—ç­‰ï¼‰
    available_actions: List[str] = field(default_factory=list)  # å¯ç”¨æ“ä½œ
    elements_count: int = 0     # æ£€æµ‹åˆ°çš„å…ƒç´ æ•°é‡

    def to_dict(self) -> Dict:
        return {
            "application": self.application,
            "window_title": self.window_title,
            "window_size": self.window_size,
            "scene_type": self.scene_type,
            "available_actions": self.available_actions,
            "elements_count": self.elements_count
        }


class VisualEngine:
    """
    è§†è§‰ç†è§£å¼•æ“

    ä¸–ç•Œç¬¬ä¸€çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼š
    - æ¯«ç§’çº§å…ƒç´ æ£€æµ‹
    - è‡ªç„¶è¯­è¨€å®šä½
    - å¤šåˆ†è¾¨ç‡è‡ªé€‚åº”
    """

    def __init__(self, use_ocr: bool = True, use_ml: bool = False):
        """
        åˆå§‹åŒ–è§†è§‰å¼•æ“

        Args:
            use_ocr: æ˜¯å¦å¯ç”¨ OCR æ–‡æœ¬è¯†åˆ«
            use_ml: æ˜¯å¦ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆéœ€è¦ GPUï¼‰
        """
        self.use_ocr = use_ocr
        self.use_ml = use_ml

        # åˆå§‹åŒ– OCR
        self.ocr_engine = None
        if use_ocr:
            self._init_ocr()

        # åˆå§‹åŒ– ML æ¨¡å‹
        self.detection_model = None
        if use_ml:
            self._init_ml_model()

        # å…ƒç´ ç¼“å­˜ï¼ˆç”¨äºåŠ é€Ÿé‡å¤æ£€æµ‹ï¼‰
        self._element_cache: Optional[List[UIElement]] = None
        self._cache_timestamp: float = 0
        self._cache_duration: float = 0.5  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰

        print("[VisualEngine] åˆå§‹åŒ–å®Œæˆ")
        if use_ocr:
            print("  âœ“ OCR å·²å¯ç”¨")
        if use_ml:
            print("  âœ“ ML æ¨¡å‹å·²åŠ è½½")

    def _init_ocr(self):
        """åˆå§‹åŒ– OCR å¼•æ“"""
        try:
            import easyocr
            self.ocr_engine = easyocr.Reader(['ch_sim', 'en'])
            print("[VisualEngine] EasyOCR åŠ è½½æˆåŠŸ")
        except ImportError:
            try:
                import pytesseract
                self.ocr_engine = "tesseract"
                print("[VisualEngine] Tesseract OCR åŠ è½½æˆåŠŸ")
            except ImportError:
                print("[VisualEngine] âš ï¸ OCR åº“æœªå®‰è£…ï¼Œè·³è¿‡ OCR åŠŸèƒ½")
                self.use_ocr = False

    def _init_ml_model(self):
        """åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ£€æµ‹æ¨¡å‹"""
        try:
            # è¿™é‡Œå¯ä»¥åŠ è½½ YOLOv8 æˆ– DETR æ¨¡å‹
            # from ultralytics import YOLO
            # self.detection_model = YOLO("yolov8n-ui.pt")
            print("[VisualEngine] ML æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"[VisualEngine] âš ï¸ ML æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.use_ml = False

    def detect_elements(self, screenshot: Union[np.ndarray, Image.Image]) -> List[UIElement]:
        """
        æ£€æµ‹å±å¹•ä¸Šçš„æ‰€æœ‰ UI å…ƒç´ 

        Args:
            screenshot: å±å¹•æˆªå›¾ï¼ˆnumpy æ•°ç»„æˆ– PIL Imageï¼‰

        Returns:
            UIElement åˆ—è¡¨
        """
        # è½¬æ¢å›¾åƒæ ¼å¼
        if isinstance(screenshot, Image.Image):
            img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)
        else:
            img = screenshot.copy()

        elements = []

        # 1. ä½¿ç”¨ä¼ ç»Ÿ CV æ–¹æ³•æ£€æµ‹åŸºç¡€å…ƒç´ 
        cv_elements = self._detect_with_cv(img)
        elements.extend(cv_elements)

        # 2. ä½¿ç”¨ ML æ¨¡å‹æ£€æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_ml and self.detection_model:
            ml_elements = self._detect_with_ml(img)
            elements.extend(ml_elements)

        # 3. OCR è¯†åˆ«æ–‡æœ¬
        if self.use_ocr:
            self._enrich_with_ocr(img, elements)

        # 4. è¿‡æ»¤å’Œæ’åºï¼ˆæŒ‰ç½®ä¿¡åº¦ï¼‰
        elements = self._filter_and_sort(elements)

        return elements

    def _detect_with_cv(self, img: np.ndarray) -> List[UIElement]:
        """ä½¿ç”¨ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³•æ£€æµ‹å…ƒç´ """
        elements = []
        height, width = img.shape[:2]

        # 1. æ£€æµ‹æŒ‰é’®ï¼ˆåœ†è§’çŸ©å½¢ç‰¹å¾ï¼‰
        buttons = self._detect_buttons(img)
        elements.extend(buttons)

        # 2. æ£€æµ‹è¾“å…¥æ¡†
        inputs = self._detect_inputs(img)
        elements.extend(inputs)

        # 3. æ£€æµ‹å›¾æ ‡ï¼ˆå°å°ºå¯¸æ–¹å½¢åŒºåŸŸï¼‰
        icons = self._detect_icons(img)
        elements.extend(icons)

        # 4. æ£€æµ‹æ–‡æœ¬åŒºåŸŸ
        texts = self._detect_text_regions(img)
        elements.extend(texts)

        return elements

    def _detect_buttons(self, img: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹æŒ‰é’®"""
        elements = []

        # é¢„å¤„ç†
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹æ‰¾çŸ©å½¢
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)

            # è¿‡æ»¤å¤ªå°çš„åŒºåŸŸ
            if w < 40 or h < 20:
                continue

            # è¿‡æ»¤æ¯”ä¾‹ä¸åˆç†çš„ï¼ˆæŒ‰é’®é€šå¸¸æ˜¯æ‰å¹³çš„ï¼‰
            ratio = w / h
            if ratio < 1.5 or ratio > 8:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯åœ†è§’ï¼ˆç®€åŒ–æ£€æµ‹ï¼‰
            area = cv2.contourArea(cnt)
            rect_area = w * h
            if area / rect_area < 0.6:  # å¡«å……ç‡æ£€æŸ¥
                continue

            element = UIElement(
                type=ElementType.BUTTON,
                x=x + w // 2,
                y=y + h // 2,
                width=w,
                height=h,
                confidence=0.7,
                description=f"Button ({w}x{h})"
            )
            elements.append(element)

        return elements

    def _detect_inputs(self, img: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹è¾“å…¥æ¡†"""
        elements = []

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # æŸ¥æ‰¾æ°´å¹³çº¿æ¡ï¼ˆè¾“å…¥æ¡†çš„ç‰¹å¾ï¼‰
        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 1))
        horizontal = cv2.morphologyEx(gray, cv2.MORPH_OPEN, horizontal_kernel)

        # äºŒå€¼åŒ–
        _, thresh = cv2.threshold(horizontal, 200, 255, cv2.THRESH_BINARY_INV)

        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)

            # è¾“å…¥æ¡†é€šå¸¸æ˜¯å®½è€Œæ‰çš„
            if w > 100 and h < 40 and w / h > 3:
                element = UIElement(
                    type=ElementType.INPUT,
                    x=x + w // 2,
                    y=y + h // 2,
                    width=w,
                    height=h + 20,  # å¢åŠ é«˜åº¦åŒ…å«è¾¹æ¡†
                    confidence=0.6,
                    description=f"Input field ({w}x{h})"
                )
                elements.append(element)

        return elements

    def _detect_icons(self, img: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹å›¾æ ‡ï¼ˆå°æ–¹å½¢åŒºåŸŸï¼‰"""
        elements = []

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # æŸ¥æ‰¾å°æ–¹å—
        edges = cv2.Canny(gray, 100, 200)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)

            # å›¾æ ‡é€šå¸¸æ˜¯ 16x16 åˆ° 48x48
            if 16 <= w <= 64 and 16 <= h <= 64:
                # æ¥è¿‘æ­£æ–¹å½¢
                if 0.7 <= w / h <= 1.3:
                    element = UIElement(
                        type=ElementType.ICON,
                        x=x + w // 2,
                        y=y + h // 2,
                        width=w,
                        height=h,
                        confidence=0.5,
                        description=f"Icon ({w}x{h})"
                    )
                    elements.append(element)

        return elements

    def _detect_text_regions(self, img: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹æ–‡æœ¬åŒºåŸŸï¼ˆä½¿ç”¨ MSERï¼‰"""
        elements = []

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # MSER æ£€æµ‹
        mser = cv2.MSER_create()
        regions, _ = mser.detectRegions(gray)

        for region in regions:
            x, y, w, h = cv2.boundingRect(region)

            # è¿‡æ»¤å¤ªå°çš„åŒºåŸŸ
            if w < 20 or h < 10:
                continue

            # è¿‡æ»¤ä¸åˆç†çš„æ¯”ä¾‹
            if h > w * 3:
                continue

            element = UIElement(
                type=ElementType.TEXT,
                x=x + w // 2,
                y=y + h // 2,
                width=w,
                height=h,
                confidence=0.4,
                description=f"Text region ({w}x{h})"
            )
            elements.append(element)

        return elements

    def _detect_with_ml(self, img: np.ndarray) -> List[UIElement]:
        """ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æ£€æµ‹"""
        # è¿™é‡Œæ¥å…¥ YOLOv8 æˆ–å…¶ä»–æ£€æµ‹æ¨¡å‹
        # results = self.detection_model(img)
        # è§£æç»“æœ...
        return []

    def _enrich_with_ocr(self, img: np.ndarray, elements: List[UIElement]):
        """ä½¿ç”¨ OCR å¢å¼ºå…ƒç´ ä¿¡æ¯"""
        if self.ocr_engine is None:
            return

        # å¯¹æ¯ä¸ªå…ƒç´ åŒºåŸŸè¿›è¡Œ OCR
        for element in elements:
            if element.type in [ElementType.BUTTON, ElementType.TEXT]:
                x1, y1, x2, y2 = element.bbox

                # è£å‰ªåŒºåŸŸ
                roi = img[y1:y2, x1:x2]
                if roi.size == 0:
                    continue

                # OCR è¯†åˆ«
                try:
                    if isinstance(self.ocr_engine, str) and self.ocr_engine == "tesseract":
                        import pytesseract
                        text = pytesseract.image_to_string(roi, lang='chi_sim+eng')
                        element.text = text.strip()
                    else:
                        # EasyOCR
                        results = self.ocr_engine.readtext(roi)
                        texts = [r[1] for r in results]
                        element.text = " ".join(texts)
                except Exception as e:
                    pass

    def _filter_and_sort(self, elements: List[UIElement]) -> List[UIElement]:
        """è¿‡æ»¤é‡å å…ƒç´ å¹¶æŒ‰ç½®ä¿¡åº¦æ’åº"""
        # å»é™¤é‡å å…ƒç´ ï¼ˆä¿ç•™ç½®ä¿¡åº¦é«˜çš„ï¼‰
        filtered = []
        for elem in elements:
            overlap = False
            for existing in filtered:
                # è®¡ç®— IoU
                iou = self._calculate_iou(elem.bbox, existing.bbox)
                if iou > 0.5:  # é‡å è¶…è¿‡ 50%
                    overlap = True
                    # ä¿ç•™ç½®ä¿¡åº¦é«˜çš„
                    if elem.confidence > existing.confidence:
                        filtered.remove(existing)
                        filtered.append(elem)
                    break
            if not overlap:
                filtered.append(elem)

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        filtered.sort(key=lambda x: x.confidence, reverse=True)

        return filtered

    def _calculate_iou(self, box1: Tuple[int, ...], box2: Tuple[int, ...]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„ IoU"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2

        # è®¡ç®—äº¤é›†
        xi1 = max(x1_1, x1_2)
        yi1 = max(y1_1, y1_2)
        xi2 = min(x2_1, x2_2)
        yi2 = min(y2_1, y2_2)

        inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)

        # è®¡ç®—å¹¶é›†
        box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)
        box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = box1_area + box2_area - inter_area

        return inter_area / union_area if union_area > 0 else 0

    def locate_element(self, description: str, screenshot: Union[np.ndarray, Image.Image]) -> Optional[UIElement]:
        """
        æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å®šä½å…ƒç´ 

        Args:
            description: å…ƒç´ æè¿°ï¼Œå¦‚ï¼š
                - "ä¿å­˜æŒ‰é’®"
                - "å³ä¸Šè§’çš„è®¾ç½®å›¾æ ‡"
                - "åä¸º'ç”¨æˆ·å'çš„è¾“å…¥æ¡†"
            screenshot: å±å¹•æˆªå›¾

        Returns:
            æœ€åŒ¹é…çš„å…ƒç´ ï¼Œæœªæ‰¾åˆ°è¿”å› None
        """
        elements = self.detect_elements(screenshot)

        # è§£ææè¿°
        description_lower = description.lower()

        candidates = []

        for elem in elements:
            score = 0.0

            # 1. æ–‡æœ¬åŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
            if elem.text and any(word in elem.text.lower() for word in description_lower.split()):
                score += 0.5

            # 2. ç±»å‹åŒ¹é…
            if "æŒ‰é’®" in description_lower and elem.type == ElementType.BUTTON:
                score += 0.2
            elif "è¾“å…¥" in description_lower and elem.type == ElementType.INPUT:
                score += 0.2
            elif "å›¾æ ‡" in description_lower and elem.type == ElementType.ICON:
                score += 0.2

            # 3. ä½ç½®åŒ¹é…
            img_height = screenshot.height if isinstance(screenshot, Image.Image) else screenshot.shape[0]
            img_width = screenshot.width if isinstance(screenshot, Image.Image) else screenshot.shape[1]

            if "ä¸Š" in description_lower and elem.y < img_height * 0.3:
                score += 0.1
            elif "ä¸‹" in description_lower and elem.y > img_height * 0.7:
                score += 0.1
            elif "å·¦" in description_lower and elem.x < img_width * 0.3:
                score += 0.1
            elif "å³" in description_lower and elem.x > img_width * 0.7:
                score += 0.1

            # 4. ç½®ä¿¡åº¦åŠ æƒ
            score += elem.confidence * 0.2

            if score > 0:
                candidates.append((elem, score))

        # è¿”å›å¾—åˆ†æœ€é«˜çš„
        if candidates:
            candidates.sort(key=lambda x: x[1], reverse=True)
            return candidates[0][0]

        return None

    def understand_scene(self, screenshot: Union[np.ndarray, Image.Image]) -> SceneContext:
        """
        ç†è§£å½“å‰åœºæ™¯

        Args:
            screenshot: å±å¹•æˆªå›¾

        Returns:
            SceneContext åœºæ™¯ä¸Šä¸‹æ–‡
        """
        # è½¬æ¢å›¾åƒ
        if isinstance(screenshot, Image.Image):
            img_array = np.array(screenshot)
            img_size = screenshot.size
        else:
            img_array = screenshot
            img_size = (screenshot.shape[1], screenshot.shape[0])

        # æ£€æµ‹æ‰€æœ‰å…ƒç´ 
        elements = self.detect_elements(img_array)

        # åˆ†æåœºæ™¯ç±»å‹
        scene_type = self._classify_scene(elements)

        # æ¨æ–­å¯ç”¨æ“ä½œ
        available_actions = self._infer_actions(elements)

        context = SceneContext(
            application="Unknown",  # å¯é€šè¿‡çª—å£APIè·å–
            window_title="",  # å¯é€šè¿‡çª—å£APIè·å–
            window_size=img_size,
            scene_type=scene_type,
            available_actions=available_actions,
            elements_count=len(elements)
        )

        return context

    def _classify_scene(self, elements: List[UIElement]) -> str:
        """åˆ†ç±»åœºæ™¯ç±»å‹"""
        # ç»Ÿè®¡å…ƒç´ ç±»å‹
        type_counts = {}
        for elem in elements:
            type_counts[elem.type] = type_counts.get(elem.type, 0) + 1

        # å¯å‘å¼è§„åˆ™
        if type_counts.get(ElementType.INPUT, 0) >= 2 and type_counts.get(ElementType.BUTTON, 0) >= 1:
            return "form"  # è¡¨å•é¡µ

        if type_counts.get(ElementType.BUTTON, 0) > 5:
            return "dashboard"  # ä»ªè¡¨æ¿

        if type_counts.get(ElementType.DIALOG, 0) > 0:
            return "dialog"  # å¯¹è¯æ¡†

        if len(elements) < 5:
            return "minimal"  # æç®€ç•Œé¢

        return "general"

    def _infer_actions(self, elements: List[UIElement]) -> List[str]:
        """æ¨æ–­å¯ç”¨æ“ä½œ"""
        actions = []

        for elem in elements:
            if elem.type == ElementType.BUTTON:
                actions.append(f"click_{elem.text or 'button'}")
            elif elem.type == ElementType.INPUT:
                actions.append(f"type_in_{elem.text or 'input'}")
            elif elem.type == ElementType.CHECKBOX:
                actions.append("toggle_checkbox")
            elif elem.type == ElementType.DROPDOWN:
                actions.append("select_from_dropdown")

        return list(set(actions))  # å»é‡

    def visualize_detection(self, screenshot: Union[np.ndarray, Image.Image],
                           elements: List[UIElement],
                           highlight_element: Optional[UIElement] = None) -> Image.Image:
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ

        Args:
            screenshot: åŸå§‹æˆªå›¾
            elements: æ£€æµ‹åˆ°çš„å…ƒç´ 
            highlight_element: è¦é«˜äº®çš„ç‰¹å®šå…ƒç´ 

        Returns:
            æ ‡æ³¨åçš„å›¾åƒ
        """
        if isinstance(screenshot, np.ndarray):
            img = Image.fromarray(cv2.cvtColor(screenshot, cv2.COLOR_BGR2RGB))
        else:
            img = screenshot.copy()

        draw = ImageDraw.Draw(img)

        # é¢œè‰²æ˜ å°„
        colors = {
            ElementType.BUTTON: "#FF6B6B",
            ElementType.INPUT: "#4ECDC4",
            ElementType.ICON: "#FFE66D",
            ElementType.TEXT: "#95E1D3",
            ElementType.UNKNOWN: "#CCCCCC"
        }

        for elem in elements:
            x1, y1, x2, y2 = elem.bbox
            color = colors.get(elem.type, "#CCCCCC")

            # å¦‚æœæ˜¯é«˜äº®å…ƒç´ ï¼Œä½¿ç”¨ç‰¹æ®Šé¢œè‰²
            if highlight_element and elem == highlight_element:
                color = "#FF0000"
                draw.rectangle([x1-2, y1-2, x2+2, y2+2], outline=color, width=3)
            else:
                draw.rectangle([x1, y1, x2, y2], outline=color, width=2)

            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{elem.type.value}"
            if elem.text:
                label += f": {elem.text[:15]}"

            draw.text((x1, y1 - 15), label, fill=color)

        return img


# ä¾¿æ·å‡½æ•°
def quick_detect(screenshot_path: str) -> List[Dict]:
    """å¿«é€Ÿæ£€æµ‹å›¾åƒä¸­çš„å…ƒç´ """
    engine = VisualEngine(use_ocr=False)
    img = Image.open(screenshot_path)
    elements = engine.detect_elements(img)
    return [e.to_dict() for e in elements]


def quick_locate(description: str, screenshot_path: str) -> Optional[Dict]:
    """å¿«é€Ÿå®šä½å…ƒç´ """
    engine = VisualEngine(use_ocr=True)
    img = Image.open(screenshot_path)
    element = engine.locate_element(description, img)
    return element.to_dict() if element else None


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python visual_engine.py <screenshot_path>")
        print("       python visual_engine.py <screenshot_path> <description>")
        sys.exit(1)

    screenshot_path = sys.argv[1]

    if not os.path.exists(screenshot_path):
        print(f"Error: File not found: {screenshot_path}")
        sys.exit(1)

    engine = VisualEngine(use_ocr=True)
    img = Image.open(screenshot_path)

    if len(sys.argv) >= 3:
        # å®šä½æ¨¡å¼
        description = sys.argv[2]
        print(f"Looking for: {description}")
        element = engine.locate_element(description, img)
        if element:
            print(f"Found: {element.to_dict()}")
            # å¯è§†åŒ–
            vis_img = engine.visualize_detection(img, [element], highlight_element=element)
            vis_img.save("located.png")
            print("Saved visualization to located.png")
        else:
            print("Not found")
    else:
        # æ£€æµ‹æ¨¡å¼
        print("Detecting elements...")
        elements = engine.detect_elements(img)
        print(f"Found {len(elements)} elements:")
        for i, elem in enumerate(elements[:20]):  # åªæ˜¾ç¤ºå‰20ä¸ª
            print(f"  {i+1}. {elem}")

        # å¯è§†åŒ–
        vis_img = engine.visualize_detection(img, elements)
        vis_img.save("detected.png")
        print("Saved visualization to detected.png")
